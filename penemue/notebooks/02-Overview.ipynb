{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "An overview of the data collected during our monitoring period. \n",
    "\n",
    "---\n",
    "\n",
    "In this notebook we aim to investigate the distribution of tweets, retweets, replies, hashtags and links within our dataset. By doing this we hope to gain some understanding of the data we have collected. It will also allow us to determine how active the two distinct groups of users, journalists and news organisations, are on social media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import bs4\n",
    "import re\n",
    "\n",
    "# add penemue to path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "from utils import twiterate\n",
    "from utils import Collect\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by loading the user profiles, from the Twitter API, of each user contained within the original set of Twitter lists that we provided to Penemue when we begun our data collection. We then extract only the `id_str` of each user, classifying them into their two distinct groups using the appropriately named variables, journalists and organisations.\n",
    "\n",
    "To do this we make use of the `Collect` class within the Penemue `utils`. By passing it a list of strings containing URLs of the Twitter lists we can begin to extract the user profiles by making rate limited calls to the Twitter API. Once the profiles have been collected, we then retrieve them using the `members` property of the class. As you can see below, our Twitter list URLs are stored in a JSON file so we must first open the appropriate file, then pass its contents to the `Collect` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "j = json.load(open('../data/journalists.json'))\n",
    "journalists = [user['id_str'] for user in Collect(lists=j).members]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "o = json.load(open('../data/organisations.json'))\n",
    "organisations = [user['id_str'] for user in Collect(lists=o).members]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have established our users of interest we can begin extracting some figures from the data. In order to present this data we'll define a couple of functions to chart the data as a pie chart of the distribution and a bar chart showing the top 10 occurances in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pie(joi, ooi):\n",
    "    # calculate mean percentage\n",
    "    joi_size = len(joi)\n",
    "    ooi_size = len(ooi)\n",
    "    total = joi_size + ooi_size\n",
    "\n",
    "    joi_mean = (joi_size / total) * 100\n",
    "    ooi_mean = (ooi_size / total) * 100\n",
    "\n",
    "    # data to plot\n",
    "    sizes = [joi_mean, ooi_mean]\n",
    "    labels = \"Journalists\", \"Organisations\"\n",
    "    colors = ['lightskyblue', 'lightcoral']\n",
    "    \n",
    "    # plot\n",
    "    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', \n",
    "        shadow=True, startangle=90)\n",
    "\n",
    "    plt.axis('equal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bar(joi, ooi, label):\n",
    "    # create list of users\n",
    "    id_strs = [\"@%s\" % screen_name for screen_name in joi]\n",
    "    id_strs += [\"@%s\" % screen_name for screen_name in ooi]\n",
    "\n",
    "    # count occurances\n",
    "    counter = Counter(id_strs)\n",
    "    most_common = counter.most_common(10)\n",
    "\n",
    "    # data to plot\n",
    "    labels, y = zip(*most_common)\n",
    "    x = range(len(labels))\n",
    "    \n",
    "    # plot\n",
    "    plt.bar(x, y, alpha=0.5)\n",
    "    plt.xticks(x, labels, rotation='90')\n",
    "    plt.ylabel(label)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by establishing the number of _original tweets_ authored by either a journalist of interest (`joi`) or a news organisation of interest (`ooi`) during the period of data collection. An original tweet is one that has been written and published by a user, such that it is not a reply nor a retweet.\n",
    "\n",
    "In order to do this we make use of the `twiterate` function found in the Penemue `utils`. This function allows us to iterate through a JSON file of any size containing any number of [Tweet objects](https://dev.twitter.com/overview/api/tweets) by making use of a callback function. This callback accepts a single `tweet object` as a parameter and should return the attribute(s) of the tweet that we require. The reason for using such an approach is to avoid loading the full list of tweets into memory, which may lead to an out of memory exception as the number of tweets grows.\n",
    "\n",
    "To determine whether a tweet is an _original tweet_ we must first check to see whether the tweet is a reply to another tweet and then check if the tweet is a retweet. As you can see in the callback function defined below, we do this using the `in_reply_to_status_id_str` attribute and the `retweeted_status` attribute of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_original(tweet, search):\n",
    "    if (tweet[\"user\"][\"id_str\"] in search \n",
    "        and tweet[\"in_reply_to_status_id_str\"] is None \n",
    "        and \"retweeted_status\" not in tweet):\n",
    "            return tweet[\"user\"][\"screen_name\"]\n",
    "\n",
    "joi_originals = twiterate(lambda tweet : get_original(tweet, journalists))\n",
    "ooi_originals = twiterate(lambda tweet : get_original(tweet, organisations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pie(joi_originals, ooi_originals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bar(joi_originals, ooi_originals, label=\"Original Tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will establish the number of retweets created by our users of interest throughout the period of data collection. To do this we define a new callback function that will this time only look at the `retweeted_status` of the tweet. According to the [Twitter documentation](https://dev.twitter.com/overview/api/tweets) a retweet can be identified by the presence of the `retweeted_status` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_retweet(tweet, search):\n",
    "    if (tweet[\"user\"][\"id_str\"] in search \n",
    "        and \"retweeted_status\" in tweet):\n",
    "            return tweet[\"user\"][\"screen_name\"]\n",
    "\n",
    "joi_retweets = twiterate(lambda tweet : get_retweet(tweet, journalists))\n",
    "ooi_retweets = twiterate(lambda tweet : get_retweet(tweet, organisations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pie(joi_retweets, ooi_retweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bar(joi_retweets, ooi_retweets, label=\"Retweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the process above, we will now determine the number of direct replies to other tweets were created during the period of data collection. To this we will define a callback that examines the `in_reply_to_status_id_str` attribute of a tweet. If the attribute is not `None` then we can classify it as a reply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_reply(tweet, search):\n",
    "    if (tweet[\"user\"][\"id_str\"] in search \n",
    "        and tweet[\"in_reply_to_status_id_str\"] is not None):\n",
    "            return tweet[\"user\"][\"screen_name\"]\n",
    "\n",
    "joi_replies = twiterate(lambda tweet : get_reply(tweet, journalists))\n",
    "ooi_replies = twiterate(lambda tweet : get_reply(tweet, organisations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pie(joi_replies, ooi_replies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bar(joi_replies, ooi_replies, label=\"Replies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that we have an idea of how active our users of interest are on social media, we thought it would be interesting to extract the most common content shared throughout the data collection period. Below we have extracted the top 10 links from all tweets in the dataset as well as the top 10 hashtags.\n",
    "\n",
    "To extract the link from each tweet in the dataset we again look to the `twiterate` function. We must therefore define an appropriate callback to examine each tweet. Below you will notice that we are examining the `entities` attribute of the tweet to extract the `expanded_url`. For more information on how Twitter stores its tweet entities please see the [entities documentation](https://dev.twitter.com/overview/api/entities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_url(tweet):\n",
    "    for url in tweet[\"entities\"][\"urls\"]:\n",
    "        return url[\"expanded_url\"]\n",
    "\n",
    "urls = twiterate(get_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have extracted all of the links we must then establish the top 10 links that were shared and collect their associated title (i.e. the HTML `<title>` tag associated with that webpage). To do this we are going to define a new function `get_title` which we will call for the 10 most common links. This function will load the webpage using the provided link and extract the contents of its `<title>` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_title(url):\n",
    "    # get title text\n",
    "    html = requests.get(url)\n",
    "    page = bs4.BeautifulSoup(html.text, \"html.parser\")\n",
    "    title = page.title.string if page.title != None else \"\"\n",
    "    # remove markdown grammar\n",
    "    title = re.sub(r\"\\r|\\n|\\||\\s+\", \" \", title)\n",
    "    # remove leading & trailing whitespace\n",
    "    title = title.lstrip().rstrip()\n",
    "    \n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lc = collections.Counter(urls)\n",
    "mcl = [(url, get_title(url), occ) \n",
    "       for (url, occ) in lc.most_common(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(mcl, \n",
    "             range(1, len(mcl) + 1), \n",
    "             ['Link', 'Title', 'Occurances'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame([len(urls), len(set(urls))], \n",
    "             ['Total', 'Unique'], \n",
    "             ['Links'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will extract the top 10 hashtags from the dataset. To do this we simply look at the hashtag attribute within the entities object of the tweet and count the occurences of each hashtag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hashtag(tweet):\n",
    "    for hashtags in tweet[\"entities\"][\"hashtags\"]:\n",
    "        return hashtags[\"text\"]\n",
    "    \n",
    "hashtags = twiterate(get_hashtag)\n",
    "hc = Counter(hashtags)\n",
    "mch = [(\"#\" + key, value) \n",
    "       for (key, value) in hc.most_common(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels, y = zip(*most_common)\n",
    "x = range(len(labels))\n",
    "\n",
    "plt.bar(x, y, alpha=0.5)\n",
    "plt.xticks(x, labels, rotation='90')\n",
    "plt.ylabel(\"Tweets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the data above might not provide us with any insight into the activity of our users of interest, it clearly highlights the discussions that took place during our monitoring window. With some further analysis, such as grouping journalists by hashtags, we may be able to build up a detailed picture about what exactly journalists are talking about and whether journalists stick to their domain of reporting or whether there is some cross over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
